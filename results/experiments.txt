New Experiments
nTest 7: lr = 5e-6, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 3, accumulate_grad_batches = 1
nTest 8: lr = 5e-6, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 3, accumulate_grad_batches = 2
nTest 9: lr = 5e-6, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 3, accumulate_grad_batches = 4

temp1: trained on era16_3
temp2: trained on simsat
temp3: trained on simsat_era
temp4: trained on era

Final:
test 1: lr = 5e-4, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 2
test 2: lr = 5e-5, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 2
test 3: lr = 5e-4, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 1
test 4: lr = 5e-4, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 4
test 5: lr = 5e-5, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 1
test 6: lr = 5e-5, batch_size = 2 on each GPU, 8 GPUS, deepspeed_level = 2, early_stopping patience = 5, accumulate_grad_batches = 4
